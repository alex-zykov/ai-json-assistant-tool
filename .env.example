# AI-JSON Environment Configuration
# Copy this file to .env and customize the values

# ==============================================================================
# API KEYS (REQUIRED)
# ==============================================================================

# OpenAI API Key - Get from https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key

# Anthropic (Claude) API Key - Get from https://console.anthropic.com/
ANTHROPIC_API_KEY=your-anthropic-api-key

# Mistral AI API Key - Get from https://console.mistral.ai/
MISTRAL_API_KEY=your-mistral-api-key

# LlamaAI API Key - Get from https://www.llama-api.com/
LLAMAAI_API_KEY=your-llamaai-api-key

# ==============================================================================
# APPLICATION CONFIGURATION
# ==============================================================================

# Default AI model to use across all commands
# Run "ai-json models" to see all available models
AI_MODEL=gpt-4o-mini

# File paths for configuration files (relative to working directory)
SCHEMA_PATH=./schema.json
PROMPT_PATH=./prompt.md

# ==============================================================================
# OPENAI PROVIDER CONFIGURATION (OPTIONAL)
# ==============================================================================

# Custom base URL for OpenAI API (useful for proxies or compatible APIs)
OPENAI_BASE_URL=https://api.openai.com/v1

# Sampling temperature (0.0 to 2.0) - higher values = more random
OPENAI_TEMPERATURE=0.7

# Maximum tokens to generate in response
OPENAI_MAX_TOKENS=2048

# Nucleus sampling parameter (0.0 to 1.0)
OPENAI_TOP_P=0.9

# Presence penalty (-2.0 to 2.0) - positive values encourage new topics
OPENAI_PRESENCE_PENALTY=0

# Frequency penalty (-2.0 to 2.0) - positive values reduce repetition
OPENAI_FREQUENCY_PENALTY=0

# Random seed for deterministic outputs (optional)
# OPENAI_SEED=12345

# Organization ID (if you belong to multiple organizations)
# OPENAI_ORGANIZATION_ID=org-xxxxxxx

# Project ID (if you have multiple projects)
# OPENAI_PROJECT_ID=proj-xxxxxxx

# ==============================================================================
# CLAUDE PROVIDER CONFIGURATION (OPTIONAL)
# ==============================================================================

# Maximum tokens to generate in response
CLAUDE_MAX_TOKENS=2048

# ==============================================================================
# MISTRAL PROVIDER CONFIGURATION (OPTIONAL)
# ==============================================================================

# Sampling temperature (0.0 to 2.0) - higher values = more random
MISTRAL_TEMPERATURE=0.7

# Maximum tokens to generate in response
MISTRAL_MAX_TOKENS=2048

# Nucleus sampling parameter (0.0 to 1.0)
MISTRAL_TOP_P=0.9

# Random seed for deterministic outputs (optional)
# MISTRAL_RANDOM_SEED=12345

# ==============================================================================
# LLAMAAI PROVIDER CONFIGURATION (OPTIONAL)
# ==============================================================================

# Custom base URL for LlamaAI API
LLAMAAI_BASE_URL=https://api.llama-api.com

# Sampling temperature (0.0 to 2.0) - higher values = more random
LLAMAAI_TEMPERATURE=0.7

# Maximum tokens to generate in response
LLAMAAI_MAX_TOKENS=2048

# Nucleus sampling parameter (0.0 to 1.0)
LLAMAAI_TOP_P=0.9

# Presence penalty (-2.0 to 2.0) - positive values encourage new topics
LLAMAAI_PRESENCE_PENALTY=0

# Frequency penalty (-2.0 to 2.0) - positive values reduce repetition
LLAMAAI_FREQUENCY_PENALTY=0

# Random seed for deterministic outputs (optional)
# LLAMAAI_SEED=12345

# Organization ID (optional)
# LLAMAAI_ORGANIZATION_ID=org-xxxxxxx

# Project ID (optional)
# LLAMAAI_PROJECT_ID=proj-xxxxxxx

# ==============================================================================
# SERVER CONFIGURATION (SERVER MODE ONLY)
# ==============================================================================

# Port for the web server to listen on
PORT=3000

# CORS Configuration - comma-separated origins, or * for all
# Examples:
#   CORS_ORIGINS=*                                          # Allow all origins
#   CORS_ORIGINS=http://localhost:3000                      # Single origin
#   CORS_ORIGINS=http://localhost:3000,https://myapp.com    # Multiple origins
CORS_ORIGINS=http://localhost:3000,http://localhost:3001

# Rate limiting configuration
RATE_LIMIT_WINDOW=900000  # Rate limit window in milliseconds (15 minutes)
RATE_LIMIT_MAX=100        # Maximum requests per window per IP

# ==============================================================================
# EXAMPLE USAGE
# ==============================================================================

# CLI Commands (will use the configuration above):
#   ai-json ask "What should I eat for lunch?"
#   ai-json bulk input.txt
#   ai-json test test.csv
#   ai-json server

# Override specific settings via command line:
#   ai-json ask -m claude-3-opus-latest "Generate a story"
#   ai-json server --port 8080 --model mistral-large-latest

# Check your configuration:
#   ai-json env                    # Show current settings
#   ai-json env --validate         # Validate files and API keys
#   ai-json env --config           # Show detailed provider configuration

# ==============================================================================
# NOTES
# ==============================================================================

# - Copy this file to .env and set your actual API keys
# - All file paths are relative to the working directory where you run commands
# - Environment variables take precedence over default values
# - Command-line arguments take precedence over environment variables
# - Only set the API keys for providers you plan to use
# - Keep your .env file secure and never commit it to version control
# - Use different .env files for different environments (dev, staging, prod)
